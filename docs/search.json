[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Remote Sensing Learning Diary",
    "section": "",
    "text": "My name is Mindy and I’m a Master’s student of Urban Spatial Science in the Center for Advanced Spatial Analysis (CASA). My background is in web development (full stack) and data engineering.\nThis book is made to document my weekly learning journey through CASA0023: Remotely Sensing Cities and Environments. Navigate through each week using the menu on the left side."
  },
  {
    "objectID": "week_1.html",
    "href": "week_1.html",
    "title": "1  Week 1: Getting Started with Remote Sensing",
    "section": "",
    "text": "* [make a table]\n* Passive - no emmision. uses available energy. detects reflected energy\n* Active - emits electromagnetic waves then waits to receive them back. can go through clouds. SAR is a type of active sensor.\n\n\n\n* Radiance \n    * a property of the sensor — how much energy it’s collecting\n    * dependent on light source\n* Reflectance\n    * ratio of light coming to and leaving\n    * it’s a property of a material (like reflectance of grass)\n    * surface reflectance removes effects of light source **and** atmosphere\n\n\n\n(make this a table too) 1. spatial * size of raster cell (ex: 10m. RBG are all 10m bands) 2. spectral * electromagnetic spectrum * number of bands (ex: RGB is 3 bands) 3. radiometric * bits * granularity of energy 4. temporal * time of sensor revisiting\n*Can downscale or upscale bands to convert them to diff spatial resolution so we can use more of the spectral data. downscaling is intensive but upscaling (aka resampling) can be done with (1) knn or (2) majority rule.\nEvery feature on earth’s surface has a spectral signature which is the electromagnetic energy it reflects at each spectral band. A few of these bands are visible to us (Red, Blue, and Green), but most aren’t.\n\n\n\nSpectral Signatures\n\n\nSensors produce a digital number which represents the amount of energy collected by the sensor. A raw image is a 2D matrix where each pixel represents the raw DN value (one for each band i think). We have to convert these values into radiance, then into reflectance (2 types: top of atmosphere and bottom of atmosphere aka surface reflectance). Surface reflectance is the true property of the object (ratio of energy reflected by the object. The DN values are not the true value because the sensor might capture different amounts of energy on different days based on weather conditions.\nThe spectral signature is also known as the spectral reflectance. It shows the reflectance ratio for each band (wavelength) for the feature. We know we’re getting the surface reflectance because the data we took from Sentinel 2 and Landsat 8 products have been processed to provide BOA surface reflectance. If a band has a low reflectance value it means that that surface type is absorbing the energy (like water absorbs almost all infrared waves, whereas vegetation highly reflects infrared). Vegetation looks green to us because it reflects a lot of green light.\nDifferent sensors might give slightly different spectral signatures for the same area of the earth’s surface. This is because they may have different spectral resolutions/sensitivity. The practical is about comparing spectral signatures between Sentinel 2 and Landsat 8 data.\n\n\n\n\nI chose to look at a region of Singapore for this practical.\nbands \nColor Composites (1) True Colour Composite (B1=Blue, B2=Green, B3=Red) [insert image]  (2) False Color Composite (B8, B4, B3) - Plants reflect near-infrared and green light whilst absorbing red.  (3) Atmospheric Penetration Composite - B12, B11, B8A with no visible bands to penetrate atmospheric particles. Vegetation = blue, urban area = white, gray cyan or purple.  (4) Short-wave Infrared Composite (B12, B8A and B4) - shows vegetation in green, the darker the greener the denser it is and brown shows built up or bare soil …and there are more\nSpectral Feature Space - Wizard/Tasseled Cap - x axis (brightness): B4 (red) - vegetation absorbs - y axis (greenness): B8 (near infrared reflectance) - vegetation reflects  \nTasseled Cap Function: A way of transforming original data to new data with reduced dimensionality (using PCA - reducing dimensions while maximising variance). Brightness, associated with spectral bands that show bare soil, man made surfaces or bight materials Greenness, associated with green vegetation Wetness, associated with moisture"
  },
  {
    "objectID": "week_1.html#applications",
    "href": "week_1.html#applications",
    "title": "1  Week 1: Getting Started with Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nEnvironmental Application: A study was done to assess water quality by looking for particular algae. The researchers asked what is the spectral reflectivity of this type of algae in this type of water? They use a spectroscope to emit electromagnetic radiation onto the aglae and to get the spectral signature. Then when assessing water images, can then search for pixels that are similar to this spectral signature. They write a program to find every pixel that has a a reflectivity of x in the infrared band, a reflectivity of y in the red band, …etc. (Rundquist et. al, 1996)\nMilitary application: Camouflaging equipment in colors (making them green to hide in trees, but not camouflaging in infrared would mean that if something flew over with infrared sensors, they could be detected. Mentioned in this youtube video: https://www.youtube.com/watch?v=KF2j4sH7pkE&ab_channel=Boldbayar.R\nHow can we differentiate between similar looking surfaces? If you have a higher spectral resolution (chop it up into finer bands), you can better differentiate between things that might look similar (like oak tree vs an elm tree, grass vs astroturf)\n (Lillesane et al., 2015)"
  },
  {
    "objectID": "week_1.html#reflection",
    "href": "week_1.html#reflection",
    "title": "1  Week 1: Getting Started with Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nThere is so much more to remote sensing than I expected. I had a really basic idea that there were just satellites that took pictures from outer space but I didn’t realise how so many different lengths of electromagnetic waves, beyond those which are visible to us, can give us different types of information.\nI’m interested in the use of these spectral signatures in identifying building materials as I think that could have really useful applications in climate and sustainability. I thought about this article I read from Bloomberg CityLab that assessed heat islands in major cities in relation to the concentration of specific materials and colors (clay buildings and white paint being cooler while concrete was hotter).\nI also think it’s really great that a lot of data is free and accessible, which will improve capabilites for large scale data collection and analysis in data poor areas."
  },
  {
    "objectID": "week_2.html",
    "href": "week_2.html",
    "title": "2  Week 2: Xaringan",
    "section": "",
    "text": "I made a presentation about the Sentinel-2 sensor. Click here to see my presentation"
  },
  {
    "objectID": "week_3.html",
    "href": "week_3.html",
    "title": "3  Week 3: Corrections",
    "section": "",
    "text": "In the land before time (aka 5-6 years ago?) satellite imagery needed to be pre-processed in a couple of ways before analysis to (1) correct for numerous factors that affected the accuracy and reliability of the images and (2) merge multiple images when one doesn’t cover our entire study area. I will summarise some of these issues and how to correct them. At the end, I will talk about types of image enhancements that we can perform one we have our corrected imagery.\n\n\n\n\n\natmospheric effects - distortions caused by the way light interacts with the atmosphere (mostly in the visible and near infrared bands). Haze (absorption + scattering) reduces contrast.\nsensor errors - geometric distortions (like the view angle of the sensor), radiometric calibration errors, sensor noise\nterrain effects - variation in topography (like changes in elevation and slope) can cause differences in illumination and shadows that distort the image.\nrotation of the earth - causing features on the ground to appear like they are moving\n\n\n\n\n\nGeometric correction - comparing the image to a reference data set such as a map or another image and choosing ground control points (GCPs), which are easily identifiable features like road intersections or buildings, on both items. We can then plug these coordinates into a model to find the best transformation function.\nAtmospheric correction - relative or absolute\n\nDark Object Subtraction (relative)\n\ntake a dark feature which you would expect to have a value of 0. any value it does have is then assumed to be the atmosphere. use that value as what we should subtract from all other pixels.\n\n\n\nTopographic (orthorectification) correction - making the pixels viewed from directly overhead rather than at an angle.\nRadiometric correction involves converting raw digital numbers (DN) to radiance or reflectance to correct for atmospheric or sensor-related effects.\n\nSome terms…\n\nDN - Intensity of the electromagnetic radiation per pixel. The raw value that the sensor collects.\nRadiance - how much light the sensor sees. Includes effects of light source, atmosphere, and surface material\nReflectance - the ratio of light leaving the target to amount striking the target. (property of the material). Usually BOA, aka surface reflectance.\nIrradiance - the amount of energy from the sun or another source that reaches the top of the Earth’s atmosphere.\n\n\n\n\n\n\n\nThe second topic we discussed was joining data sets.\nWhen we need to combine multiple images in order to cover our entire study area, we can use a technique called “Mosaicking”, which involves feathering the images. Two adjacent images will overlap slightly, and here, one easy solution is to take the mean of the overlapping pixels (but there are more sophisticated methods too)\n[insert practical images]\nThese images are from practical 5\n\n\n\nOnce we have our corrected imagery, we can apply enhancements highlight different types of information that might be useful to our study. Many of these techniques help improve the accuracy of land cover classification. Here are some types of enhancements:\n\nContrast enhancement - stretching the color range to improve the visual quality of the image.\nRatioing - dividing the values of two spectral bands to create a new image where the pixel values represent the ratio of the two bands. This helps highlight certain features in the image that may not be as visible in the individual bands.\nFiltering\nPCA\nMeasuring Texture\nImage fusion"
  },
  {
    "objectID": "week_3.html#applications",
    "href": "week_3.html#applications",
    "title": "3  Week 3: Corrections",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nI started looking up different use cases of the various image enhancement techniques and found the ratioing to be the most interesting. Some uses:\n\nDetecting vegetation health\nSoil moisture mapping\nMineral mapping\nUrban heat island analysis\nWater quality monitoring\nFlood mapping\nCostal zone mapping\n\nI thought the example used in the policy lecture about seeing who is still watering their lawn during a drought was a really amusing use case, especially because I am from California and have driven through neighborhoods where there is a clear and visible difference between who is abiding by the lawn watering ban from one house to the next. I’ve also been frustrated when driving past huge and mostly empty golf courses that remain bright green year round.\nThis is an example application of detecting soil moisture health, comparing dead dry lawns to healthy moist ones. The Normaliszed Difference Moister Index (NDMI) is calculated using this equation :\n\\[\nNDMI=(B08−B11)/(B08+B11)\n\\] And it uses bands B08 (near infrared) and B11 (short wave infrared) from Sentinel-2 (note that it would take different bands when using data from different sensors). B08 is good for highlighting dense vegetation that appears as dark green. B11 is useful for vegetation moisture content (which means it’s also useful for mapping forest fires!).\nIn the image below, the moist soil is indicated in blue while the dry is in red:  (source: (ai6yrham?))\nI was surprised by another image in this thread which shows some of the bigger golf courses cutting back on their watering.  (source: (ai6yrham?))"
  },
  {
    "objectID": "week_3.html#reflection",
    "href": "week_3.html#reflection",
    "title": "3  Week 3: Corrections",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nAlthough we most likely will not have to correct our own imagery in most cases, since most data comes corrected already it’s important to have a general understanding of how these values are generated for a couple of reasons.\n\ngood to have background knowledge incase something is behaving differently / you’re getting different results than how you would expect\nat some point we might receive data that isn’t already corrected. although i can guarantee you I will not be able to remember exactly how to correct what type of data in what way just from this one lecture, at least I know what to look up.\n\nWhenever technology advances this much so that processes that used to take days now can be done in a few lines of code, I wonder how much people in a field could have achieved if they could have used more of their time on the post processing steps. But I guess that’s always how it goes — we do things the hard, manual way until someone figures out how to automate it, then we continue on to the next hard thing.\nI appreciate that imagery comes corrected now because it lowers the barrier for entry. Although i find remote sensing data and its applications fascinating, I think i would be really deterred from continuing to study it if I had to do that much preprocessing beforehand. Because, not gonna lie, this was the most overwhelming lecture of all."
  },
  {
    "objectID": "week_4.html",
    "href": "week_4.html",
    "title": "4  Week 4: Policy",
    "section": "",
    "text": "In 2015, despite not being a member state of the UN, the State of Palestine committed to the 2030 Agenda for Sustainable Development. One of the goals of the New Urban Agenda is outlined below:\n\n“Take action to address climate change by reducing their greenhouse gas emissions”\nLeaders have committed to involve not just the local government but all actors of society to take climate action taking into account the Paris Agreement on climate change which seeks to limit the increase in global temperature to well below 2 degrees Celsius. Sustainable cities that reduce emissions from energy  and build resilience can play a lead role.”\n\nIn a report documenting their progress in implementing the New Urban Agenda from 2016 - 2021, Palestine outlines the progress made on each development theme. Under “Resilience, Mitigation and Adaptation of Cities and Human Settlements”, one of the goals is:\n\n3.1.2 Implement climate change mitigation and adaptation actions\n3.1.2.b Annual mean levels of fine particulate matter (e.g., PM2.5 and PM10) in cities (population weighted)\n\nHigh air pollution in Palestinian territories can be attributed to rapid urbanisation, the improper management of solid waste, and a growing population, much of whom rely on motor vehicles to get around due to a limited access to alternative forms of transportation. These consequences are both environmental and social, having negative impacts on liveability, quality of life, and health outcomes (including respiratory diseases, serious immune system disorders, neurological diseases, cardiovascular diseases, and lung cancer).\nIn terms of progress on this goal, the report states:\n\nNeither government authorities nor LGUs track the levels of the fine PM as part of a comprehensive system, although authorities including major municipalities acknowledge the importance of monitoring air pollution including the levels of the fine PM especially at proximity of industrial zones and quarries, in addition to various topographic locations so as to provide specific and macro-level indications about the impact of factories and transborder activities.\n\nIt’s clear from this report that monitoring air pollution in the spatial dimension is of high importance to the State of Palestine, but they lack a systematic way to do so.\ncitations!!"
  },
  {
    "objectID": "week_4.html#applications",
    "href": "week_4.html#applications",
    "title": "4  Week 4: Policy",
    "section": "4.2 Applications",
    "text": "4.2 Applications\nThe state has been experiencing a lack of funding due to both international aid declining significantly since 2019 and Israel’s witholding of tax revenue. This means that any strategies we propose must be low cost. On ground monitoring would provide reliable air quality measurements, but require costly infrastructure, installation, and management. The use of satellite imagery to map particulate matter (PM) concentrations has been well studied in recent years.\nData and Methods\nAerosol Optical Depth (AOD) is widely used to measure near-surface atmospheric PM. It is a measure of the amount of light scattered or absorbed by aerosol particles in the atmosphere. We will use imagery from MODIS (the Moderate Resolution Imaging Spectroradiometer), which provides AOD measurements at a daily temporal resolution.\nZhang et. al (2021) summarises and compares the various methods for satellite-based PM concentration mapping developed in the field over the past 20 years. The authors assessed the advantages and disadvantages of 4 different common methods:\n\nIn their discussion, they suggest a combination of the first 3 methods: univariate regression, CTM-based, and multivariate statistical.\nThe land use regression model (LUR) is another method that can be employed in urban areas. It uses variables like land use, traffic, population density, and other geographic factors around the site as predictors for PM concentration. Liu et al. (2016) applied this method in Shanghai without using any satellite AOD data and achieved an R^2 of 0.88. We can test a hybrid model using LUR with one or multiple of the methods mentioned above and find that which achieves the highest predictive accuracy for our mapping.\nOnce we have this model, we can use seasonal decomposition to see how pollution varies throughout seasons of the year. We can also conduct a time series analysis to assess how it changes day-to-day over the course of a week.\nRecommended usages\nThis data will allow planners and policy makers to understand the extent of impact that factories, quarries, and transborder activities have on air quality. They can also see where motor vehicles and improper waste management practices have the largest effects. They can use this information to target interventions in the areas with the highest exposure. Interventions could include assigning which cars can drive on which days (a successful strategy adopted by other cities including Paris and Beijing), keeping housing development away from industrial zones, implementing new waste management strategies or increasing education around the environmental impact of trash incineration. They can also use this data to (1) expose the environmental and health impacts that the illegal occupation is causing, and (2) apply pressure on international community for more aid and support.\nFuture Recommendations\nEventually a real-time air quality monitoring system would be ideal so that they implement strategies in real time, however given the limited funds, it is more feasible first step to model the spatial and temporal concentrations of PM using historical data. In the future, using data from geostationary satellites (which orbit around the equator at the same speed as earth) which have a higher temporal resolution could be a way to get closer to developing real-time monitoring system."
  },
  {
    "objectID": "week_4.html#reflection",
    "href": "week_4.html#reflection",
    "title": "4  Week 4: Policy",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nThe spatial and political arrangement of Palestine, specifically in the West Bank, presents a unique challenge to urban planning and climate mitigation strategies for many reasons. One is the lack of sovereignty over certain areas which implies an inability to implement desired strategies. Another is the fragmentation and lack of contiguity between territories — enclaves are surrounded by Israeli occupied territory and are thus exposed to the air pollution produced in those areas, especially close to the borders. Particulate matter is able to travel long distances, meaning that the Palestinian population could be exposed to pollution from activities occurring in areas that are not under Palestinian jurisdiction (i.e. Israel or Area C territories which are Israeli controlled and occupied).\nThis practical showed me how necessary it is to understand of the specific political, economic, social, and spatial characteristics of a region when coming up with effective solutions. One-size-fits all solutions do not exist and there is no universal strategy that can be implemented in every city!"
  },
  {
    "objectID": "week_1.html#overview-of-what-was-covered-in-lecture",
    "href": "week_1.html#overview-of-what-was-covered-in-lecture",
    "title": "1  Summary",
    "section": "1.1 Overview of what was covered in lecture",
    "text": "1.1 Overview of what was covered in lecture\n\n1.1.1 Active vs Passive sensors\n* [make a table]\n* Passive - no emmision. uses available energy. detects reflected energy\n* Active - emits electromagnetic waves then waits to receive them back. can go through clouds. SAR is a type of active sensor.\n\n\n1.1.2 How electromagnetic waves interact with earth’s surface and atmosphere\n* Radiance \n    * a property of the sensor — how much energy it’s collecting\n    * dependent on light source\n* Reflectance\n    * ratio of light coming to and leaving\n    * it’s a property of a material (like reflectance of grass)\n    * surface reflectance removes effects of light source **and** atmosphere\n\n\n1.1.3 Different types of resolutions\n(make this a table too) 1. spatial * size of raster cell (ex: 10m. RBG are all 10m bands) 2. spectral * electromagnetic spectrum * number of bands (ex: RGB is 3 bands) 3. radiometric * bits * granularity of energy 4. temporal * time of sensor revisiting\n*Can downscale or upscale bands to convert them to diff spatial resolution so we can use more of the spectral data. downscaling is intensive but upscaling (aka resampling) can be done with (1) knn or (2) majority rule.\nEvery feature on earth’s surface has a spectral signature which is the electromagnetic energy it reflects at each spectral band. A few of these bands are visible to us (Red, Blue, and Green), but most aren’t.\n\n\n\nSpectral Signatures\n\n\nSensors produce a digital number which represents the amount of energy collected by the sensor. A raw image is a 2D matrix where each pixel represents the raw DN value (one for each band i think). We have to convert these values into radiance, then into reflectance (2 types: top of atmosphere and bottom of atmosphere aka surface reflectance). Surface reflectance is the true property of the object (ratio of energy reflected by the object. The DN values are not the true value because the sensor might capture different amounts of energy on different days based on weather conditions.\nThe spectral signature is also known as the spectral reflectance. It shows the reflectance ratio for each band (wavelength) for the feature. We know we’re getting the surface reflectance because the data we took from Sentinel 2 and Landsat 8 products have been processed to provide BOA surface reflectance. If a band has a low reflectance value it means that that surface type is absorbing the energy (like water absorbs almost all infrared waves, whereas vegetation highly reflects infrared). Vegetation looks green to us because it reflects a lot of green light.\nDifferent sensors might give slightly different spectral signatures for the same area of the earth’s surface. This is because they may have different spectral resolutions/sensitivity. The practical is about comparing spectral signatures between Sentinel 2 and Landsat 8 data."
  },
  {
    "objectID": "week_1.html#some-learnings-from-the-practical",
    "href": "week_1.html#some-learnings-from-the-practical",
    "title": "1  Summary",
    "section": "1.2 Some learnings from the practical",
    "text": "1.2 Some learnings from the practical\nI chose to look at a region of Singapore for this practical.\nbands \nColor Composites (1) True Colour Composite (B1=Blue, B2=Green, B3=Red) [insert image]  (2) False Color Composite (B8, B4, B3) - Plants reflect near-infrared and green light whilst absorbing red.  (3) Atmospheric Penetration Composite - B12, B11, B8A with no visible bands to penetrate atmospheric particles. Vegetation = blue, urban area = white, gray cyan or purple.  (4) Short-wave Infrared Composite (B12, B8A and B4) - shows vegetation in green, the darker the greener the denser it is and brown shows built up or bare soil …and there are more\nSpectral Feature Space - Wizard/Tasseled Cap - x axis (brightness): B4 (red) - vegetation absorbs - y axis (greenness): B8 (near infrared reflectance) - vegetation reflects  \nTasseled Cap Function: A way of transforming original data to new data with reduced dimensionality (using PCA - reducing dimensions while maximising variance). Brightness, associated with spectral bands that show bare soil, man made surfaces or bight materials Greenness, associated with green vegetation Wetness, associated with moisture"
  },
  {
    "objectID": "week_5.html",
    "href": "week_5.html",
    "title": "5  Week 5: Google Earth Engine",
    "section": "",
    "text": "While we’ve spent the past few weeks learning how to correct sensor data, we’ve now been introduced to Google Earth Engine, a service for processing, analysing, and visualising geospatial data, which also provides a vast library of super massive datasets with already corrected data. It allows for processing at scale. Its user interface and development environment make for easy usage of their processing libraries, reading documentation, inspecting output, and visualisation.\nTypical processes that can be done in EE include data preprocessing (filtering by time frame, masking out clouds), geometry operations, image analysis, statistical analysis, modelling / machine learning. While EE provides many powerful tools for processing large amounts of data, it’s not as great for analysis. Analysis that might take a few lines of code in R are many more lines in javascript. Therefore, it’s recommended to get the relevant data we need from EE, then export it and do further analysis in R."
  },
  {
    "objectID": "week_7.html",
    "href": "week_7.html",
    "title": "7  Week 7: Classification 2",
    "section": "",
    "text": "This week we continued learning about classification. We considered other units to classify beyond the single pixel, as this is not often the best shape to work with.\nBoth of the following techniques:\n\nwork best with high spatial resolution imagery\nimprove the accuracy of classification by reducing noise and variability\n\n\n\nSince pixels don’t often represent an object, this technique involves creating superpixels by assessing the homogeneity of neighboring pixels and grouping them. The algorithm commonly used is called SLIC (Simple Linear Iterative Clustering). It’s similar to DBSCAN in that it finds similar (determined by setting parameter) pixels within a radius, then moves the center point to the center of those pixels. Iterations 4-10 are usually the best.\nUses:\n\nOnce we have the superpixel, we can take the mean value and classify it using other methods we learned about in Week 6\nCan also be used for object detection\n\nI liked this gif from the lecture:\n\nNote that it doesn’t consider how smaller cells might be connected to create larger objects, but there are techniques that can be used to merge cells. The decision to merge depends on the application/desired outcome.\n\n\n\nWhat if you have multiple land cover types within one pixel? This can decrease the accuracy of the classifications. Subpixel analysis aims to overcome this limitation by estimating the proportion of different land cover types within each pixel. It considers spectral signature as a linear combination of the land cover types within the pixel.\nTo do this, we need “Spectrally pure” pixels, aka end members, which we use a reference to calculate the proportions of these land covers within mixed pixels. We use an inverse of a matrix that contains the end members to “unmix” the mixed pixels.\n\nmixed pixel = end member matrix * fractions\nso… fractions = inverse end member matrix * mixed pixel\n\nOne question I have is how we would reconcile end members of the same type of land cover that have different properties? For example, “urban” can consist of many different types of materials that might have different spectral signatures. Or do we just accept that the relative difference between the spectral signatures of concrete vs steel is distinct enough from the difference they have with vegetation or soil?\n\n\n\nNext we discussed how to assess the accuracy of these classification models.\nWhen assessing accuracy, overall accuracy is often not a very useful measure. We have to find a balance between producer accuracy and user accuracy as both cannot be high simultaneously. We have to determine what is more important to us. We can also use the F1 Score which combines recall and precision, though it doesn’t consider true negatives.\n\n\n\n\n\n\n\nProducer\nUser\n\n\n\n\nMaximising true positives, minimise false positives\nMinimising false negatives\n\n\nRecall\nPrecision\n\n\nevery “plane” is actually a plane\nclouds should not be classified as planes\n\n\ntrue positives but some false positives (predicted urban but land cover that isn’t urban)\nactually urban but predicted other landcover\n\n\n\nWe also learned about best practices for selecting training/test data for these accuracy assessments. I won’t go into detail but these are what they are:\n\nTest / train split by random sampling\nCross Validation - do multiple iterations with different test train splits and take the mean accuracy\nLeave one out cross validation - repeatedly running the model on all data except for 1 observation which then gets used as the test. this allows us to maximise training data and results in high accuracy, however very computationally expensive\nspatial cross validation - dealing with spatial autocorrelation between training and test splits by using k-means clustering in each fold"
  },
  {
    "objectID": "week_5.html#applications",
    "href": "week_5.html#applications",
    "title": "5  Week 5: Google Earth Engine",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nSome of Google Earth Engines most powerful applications are within the realm of\nenvironmental monitoring:\n\ntracking land use change\nmonitoring deforestation\nair quality, surface temperatures, water quality\nflood detection\ndamage detection\nagriculture (crop patterns, soil health)\n\nAnd within the societal realm, including…\n\npopulation mapping\ndetecting informal settlements\nunderstanding patterns of urbanisation\ntraffic patterns\nusing nighttime lights to estimate GDP / economic development\n\nPerhaps one of the most useful features is the ability to make interactive Google earth engine applications that are readily usable by relevant stakeholders such as NGO’s, researchers, government, or local authorities. By providing an interface that allows users to customise parameter inputs, these apps allow these agencies to generate insights specific to their needs and can empower them to make data-driven decisions.\nHere is a cool app!\nInformal settlement detection in Dar es Salaam by Ollie Balinger:  - Uses sentinel-2 data and a random forest classifier to detect informal settlements based on roof materials and open street map data to measure housing density. - I’m currently taking another module in which we’ve studied informal settlement development in Dar es Salaam. The literature we look at is mostly qualitative. We look at case studies of specific areas and generally identify the types of areas that informal settlements occur — along major roads, at the urban edges, in the inner city. But this type of data usage and analysis allows for quantitative analysis — we can answer questions like what percentage of the settlements occur in which types of areas? In which areas should we focus the most resources?"
  },
  {
    "objectID": "week_5.html#reflection",
    "href": "week_5.html#reflection",
    "title": "5  Week 5: Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nI think it’s great that services like this exist and provide their tools and data for free because it makes the entry bar for getting into remote sensing analysis much lower, allowing more people to utilise and benefit from it. Because it has so many powerful applications within climate and social justice, it’s important that the people who work within relevant sectors have access this type of information. Traditionally agencies that work toward societal or climate related causes don’t have as much funding or technical expertise as other more lucrative sectors within tech, so Google Earth Engine makes it a lot easier for the people who need this data and the insights from it to access and learn it.\nI do think it’s important that we learned how to correct the data even though we most likely won’t need to do it ourselves in the future. For one, there is value in learning where the data we receive comes from and how it is processed beforehand (especially knowing the implications of processing it a certain way — maybe there’s a situation in which it doesn’t make sense for the type of analysis we’re doing and we’d need to process it differently). Furthermore, this is a service provided by one very powerful company, and if they decide to shut it down one day, or start charging for it (more a plausible scenario), we may need to start doing it ourselves (or to build a new product that competes with theirs 🤠)."
  },
  {
    "objectID": "week_8.html",
    "href": "week_8.html",
    "title": "8  Week 8: Temperature and Policy",
    "section": "",
    "text": "This week we learned about the heat island effect and how entities at various levels (international orgs, counties, cities) aim to tackle this issue through interventions of varying degrees of effectiveness.\nWhat is the heat island effect? Watch this short interpretive dance to find out:\n\n[insert video]\nin other words, it’s when atmospheric and surface temperatures are higher in urban areas due to more concrete and other dark surfaces which retain heat, less vegetation, buildings blocking visible sky (less dissapation of heat) the feedback loop caused by air conditioning\n\nWhat effects does it have?\n\nDisproportionate social effects on health. This paper reveals how previously redlined areas in the US are subject to higher temperatures. (cite)\nEnvironmental (more AC → more energy needed → fossil fuels burned → pollution)\nEconomic (mostly the cost of dealing with health impacts)\n\nGlobal policy documents, like the UN Sustainable Development Goals, have outlined (mostly vague) strategies for tackling the urban heat island issue.\nHere are some strategies that various cities have implemented:\n\n\n\n\n\n\n\n\n\nCity\nStrategy\nEffective?\nHow just?\n\n\n\n\nBarcelona\nSuperblocks\nIncreases pedestrian traffic, reduces noise pollution, reduces nitrogen oxide, reduces particle pollution, increases business\nJustice — transforming neighborhood. identified many neighborhoods where this could work, not just wealthy ones.\n\n\nWestern Sydney\nCool roads trail (reflective carparks to reduce surface temperatures)\nsurface temperatures don’t systematically reduce air temperature. but sheds light on how certain neighborhoods might be privy to specific building materials/layouts/plans that attract more heat\nEquitable - making reflective streets in hotter neighborhoods\n\n\nChicago and Baltimore\nTree Vouchers\nvoluntary, doesn’t mandate\nEqual - Anyone has access\n\n\nSeattle\nGreen factor scoring for new developments/plans\nThere’s no requirement for where the vegetation has to go (developers could just stuff trees in a corner which wouldn’t help reduce temperatures as much as spacing them out)\nequal? every plan moving forward will have the same requirements.\n\n\n\n\n\n\nBarcelona Super Blocks"
  },
  {
    "objectID": "week_8.html#applications",
    "href": "week_8.html#applications",
    "title": "8  Week 8: Temperature and Policy",
    "section": "8.2 Applications",
    "text": "8.2 Applications\nA paper by Li et al. (2022) outlines the disproportionate temperatures experienced by people living in previously redlined neighborhoods.\nData:\nThey use land surface temperature (LST) data collected from MODIS (Moderate Resolution Imaging Spectroradiometer) satellite which allows them to collect data from both the day and night time (since its revisit time is 1-2 times per day). Using LST over air/surface temp data from meteorological stations gives enables analysis at higher spatial resolution. In the paper, they mention that LST has been found to have a strong correlation with air temperature, which contradicts what we discussed in lecture…\nTo determine which areas were considered redlined, they used used data from the Home Owners’ Loan Corporation for creditworthiness classifications. They calculated average surface temperatures and gathered data on emergency room visits due to heat-related illnesses for each census block group in the various cities.\nMethods:\nThey started with a t-test to identify differences between neighborhoods that were more or less redlined, then used OLS to assess the relationship (once controlling for social vulnerability). To control for spatial dependence they used Moran’s I and used OLS when spatial autocorrelation was not present, and a spatial regression model when it was.\nResults:\nThey found that historically redlined neighborhoods had higher surface temperatures (both in day and nighttime) than non-redlined neighborhoods, even after controlling for various environmental and socioeconomic factors. Furthermore, residents in historically redlined neighborhoods had higher rates of heat-related emergency department visits.\nLimitations:\nThey suggest that these visits may be caused by the higher LSTs but recognise they could also be caused by other built environment and infrastructural causes such as less access to AC, fewer shaded areas, and poor housing conditions. They also recognise that emergency visits might not fully represent the amount of heat related illnesses as residents with lower socio economic status often face barriers that prevent them from visiting the emergency room (proportion of visits from hispanic residents was lower than census, most likely due to undocumented status). Also LST might not be the best proxy for how humans experience temperature.\nImplications:\n“Existing heat-hazard prevention and mitigation plans are mostly based on entire region or city-level conditions, while few policies and initiatives pay attention to the inequalities rooted in long-standing spatial patterns of disinvestment and segregation in cities,” (Johnson et al., 2021: 947). Future planning, environmental, and public health policies need to address the legacy of redlining and target resources accordingly."
  },
  {
    "objectID": "week_8.html#reflection",
    "href": "week_8.html#reflection",
    "title": "8  Week 8: Temperature and Policy",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nI didn’t see the BBC article on heat islands, but Bloomberg City Lab published a similar story and it’s one of the studies that influenced my decision to do this Masters program. There are strategies I’ve read about previously and thought “oh wow this is innovative and sounds like it would work” — for example, painting roofs a lighter color to reduce heat absorption — but it’s really interesting to look at them through the lens of justice — that even equity is not the same as justice. It’s easy to look at them now and be hyper critical and skeptical, but it also is really understandable how well-intentioned people with a lack of data or in-depth research / analysis capabilities or critical thinking might decide to implement them in their cities."
  },
  {
    "objectID": "week_7.html#applications",
    "href": "week_7.html#applications",
    "title": "7  Week 7: Classification 2",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nBeyond more accurate land cover classification, object based image analysis can also be used for object detection. One concerning application is the usage of OBIA by governmental border agencies to detect small ships (aka boats carrying irregular migrants / asylum seekers). Frontex, the European Union’s border security agency, has been using SAR images to detect these boats with much success.\nAn aside about their motivations: They claim that this detection is crucial for search and rescue efforts with the goal of saving lives. In reality, despite accurate early detection, in many cases, Frontex has failed to rescue boats in distress. Their primary purpose is to prevent irregular migration, and this type of surveillance gives insight into the new pathways being taken by migrants — making it easier block their routes and inevitably stop them.\nBack to the remote sensing — a paper published in 2019 uses OBIA with combined high-and medium-resolution optical and radar images (tested with WorldView-2, QuickBird, GeoEye-1, Sentinel-2A, COSMO-SkyMed, and Sentinel-1 data) to detect vessels. They have similar accuracy to the SUMO (Search for Unidentified Maritime Object) algorithm, used by Frontex, in terms of true positives but returns less false positives (both elements of producer accuracy). (https://www.researchgate.net/publication/330322311_Object-based_image_analysis_approach_for_vessel_detection_on_optical_and_radar_images)"
  },
  {
    "objectID": "week_7.html#reflection",
    "href": "week_7.html#reflection",
    "title": "7  Week 7: Classification 2",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nWhile it’s very impressive that we are able to detect or classify objects with such accuracy at such granular levels using these innovative methods, it’s also scary to think about how this technology might be abused and threaten human life. We spend most of our time in CASA talking about leveraging data to improve the urban experience, improve lives, save the environment, etc., but we don’t often discuss the dangerous and harmful applications of these techniques. Thinking about the historic origins of many science and technological breakthroughs, many were researched and developed under state sponsorship during wartime, for military purposes. And…. it’s honestly so sad that some of our best inventions were motivated by violence, and political/economic interests abroad rather than the desire to improve lives at home. Anyways…how do we navigate that as academics and researchers — the paper I referenced above did not have an explicit political agenda but produced results that could further implicate vulnerable populations if leveraged by the wrong people (aka border control)."
  },
  {
    "objectID": "week_6.html",
    "href": "week_6.html",
    "title": "6  Week 6: Classification 1",
    "section": "",
    "text": "This week, we learned about classification, a machine learning technique, and how it can be applied to EO images. Within remote sensing, classification usually involves the extraction of land use/land cover information from earth observation data. It’s useful for understanding LULC change over time, or looking at the relationship between LULC and other variables such as surface temperature or pollution, susceptibility to hazards.\nClassification can be supervised or unsupervised. We also have generic classifier methods and ones that are specific to remote sensing data. They are summarised in this chart I made in Miro below:\n\n\n\nNote: We need to be careful about not overfitting (when the model fits the training data too well, and is not generalisable enough to work on new input data). There are two ways: 1) limiting how much trees grow by setting a minimum number of elements a leaf should have (top down) 2) weakest link pruning (bottom up and more complicated)."
  },
  {
    "objectID": "week_6.html#applications",
    "href": "week_6.html#applications",
    "title": "6  Week 6: Classification 1",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nAs mentioned above, classification is useful for understanding LULC change over time, or looking at the relationship between LULC and other variables such as surface temperature or pollution, susceptibility to hazards.\nA study done in Amman City, Jordan provides an example of using remote sensing data and classification techniques to inform urban planning in historically data-poor environments (citation, https://www.mdpi.com/2071-1050/11/8/2260). The aim was to understand patterns and causes of urban expansion from 1987 - 2017.\nMethodology\nThey used Landsat images (6 spectral bands) from 1987, 1997, 2007, and 2017 and classified pixels into 4 land use/covers: urban area, vegetation, exposed rocks, and exposed soils. They use the maximum likelihood supervised classification algorithm, however they don’t explain why they chose it. They do however achieve an accuracy rate of above 85% for each image (range is 86.4% - 90.2%), which seems pretty decent!\nThey then employed a change detection technique between the images for each year. Here is the final output: \nFindings and Discussion\nThey found that the most rapid rate of expansion occurred during the 1987-1997 period, and both the 1997-2007 and 2007-2017 periods experienced stable growth. They tied these findings to 2 types of population growth. The first is that the Gulf War occurred in 1991, causing a huge influx of Jordanians returning to Jordan from Gulf countries. The second is the 2 major influxes of refugee populations from 1) the invasion of Iraq in 2003 and 2) the civil war in Syria in 2011. What they found was, the most urban expansion occurred after the start of the Gulf war in the first image period. However, despite the population growing significantly from the two refugee influxes, urban expansion continued at a steady rate. The returning Jordanians mostly built private houses on their own land or bought/rented houses , resulting in lots of construction. In contrast, most of the refugees that entered stayed in refugee camps near the border.\nThey identify that the main type of urban growth in the first period was concentric, expanding mainly occurs along transport infrastructure that extend from the core. The expansion was horizontal as opposed to vertical. In the second and third periods, the expansion was both concentric and multiple nuclei, with new urban centers forming. They also experienced more vertical growth.\nWhat I learned\nThis paper highlighted the importance of understanding historical and political influences on urban expansion for future planning strategy. It also demonstrated the effectiveness of using landsat imagery with the maximum likelihood algorithm to monitor urban expansion in data poor regions."
  },
  {
    "objectID": "week_6.html#reflection",
    "href": "week_6.html#reflection",
    "title": "6  Week 6: Classification 1",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nSomething interesting to me is the trade-off that we have to make between the interpretability and accuracy of classifiers: \nMy first instinct would be to choose the model with the highest accuracy, but that’s probably because I’m coming from the tech sector, where it’s common to prioritise accuracy because when the goal is profit, it doesn’t really matter how we get there. It poses a problem when models need to be explained to stakeholders or decision-makers, and is especially problematic when what these models can affect peoples lives in significant ways. For example, when companies sell recidivism models to police departments, or models to predict credit worthiness, determining peoples futures based on what they might do rather than what they have done (aka “data determinism”). Interpretability is necessary to ensure that the model is making accurate and fair predictions.\nWithin medical research, say for classifying medical diagnoses for example, accuracy is likely more important than interpretability. However, within social science research, it seems that interpretability would have a higher priority because the goal is more oriented towards understanding certain phenomena or why factors drive particular outcomes, rather than simply predicting them."
  }
]